{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  \n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "from model.sequence_prediction import greedy_decode_batch, decode_seq_str, decode_interacively\n",
    "import pdb\n",
    "from model.loss import LossCompute\n",
    "import os\n",
    "from io_.info_print import printing\n",
    "from model.seq2seq import LexNormalizer\n",
    "from model.generator import Generator\n",
    "from evaluate.interact import interact\n",
    "MAX_LEN = 20\n",
    "script_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_folder_starts_with = \"f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04\"\n",
    "#model_folder_starts_with = \"f178-DROPOUT_EVEN_INCREASE-0.1-to_sent+word+bridge_out-model_3_046c\"\n",
    "#model_folder_starts_with = \"8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout0.1_scale_aux-True_aux-0.1do_char_dec-True_char_src_atten-model_14_ad6c\"\n",
    "#model_folder_starts_with = \"a5c77\"\n",
    "#model_folder_starts_with = \"fef8-new_data-batchXdropout_char0-to_char_src-1_dir_sent-10_batch_size-model_2_51a5\"\n",
    "#model_folder_starts_with = \"e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_2_14c2\"\n",
    "#model_folder_starts_with = \"e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_True-model_3_1a3\"\n",
    "#model_folder_starts_with =\"8ce6b\"\n",
    "#model_folder_starts_with = \"8e628\"\n",
    "#model_folder_starts_with = \"84736\"\n",
    "#model_folder_starts_with = \"84736-84736-SENT_context-get_True-attention_simplifiedXdrop_out_charXdir_word1dir_word-False_aux-0do_char_dec-True_char_src_atten-model_10_b59b\"\n",
    "#model_folder_starts_with = \"84736-84736-SENT_context-get_True-attention_simplifiedXdrop_out_charXdir_word1dir_word-False_aux-0do_char_dec-False_char_src_atten-model_9_bb70\"\n",
    "#model_folder_starts_with = \"b9e49-aux_report+dense+ponderatipn+no_bucketing100-dense_dim_auxilliary0.001weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_9_0f05\"\n",
    "#model_folder_starts_with = \"97068_rioc-b91d7-aux-again-biggerREPLICATE-replicate1-2dir_word-None_aux-model_19_fb56\"\n",
    "model_folder_starts_with = \"97079_rioc-ef365-ATTbest-scale-2-True-25dir_word_encoder-all_context-att2-model_1_cade\"\n",
    "model_folder_starts_with = \"97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all models folder that starts with model_folder_starts_with\n",
    "list_all_dir = os.listdir(\"../checkpoints/\")\n",
    "list_ = [dir_ for dir_ in list_all_dir if dir_.startswith(model_folder_starts_with) and not dir_.endswith(\"log\") and not dir_.endswith(\"summary\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to interact with \n",
    "\n",
    "#### Former models : trained on Liu only , source added as concatanation of word and sentence level as h_0 of decoder , batch size = 10 , small drop out , \n",
    "\n",
    "- f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04 : very bad at interacting (NB ; pb of eval)\n",
    "- f178 also good model trained on liu only \n",
    "- 8e628 : attention ; no aux (no bucket , get_batch False ) :  - attention degrades abit the results (still feedin char embedding also)\n",
    "- e390 + same 24f94 goo : \n",
    "    - best is e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_2_14c2 \n",
    "    - same with attentin but lame : e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_True-model_3_1a3f (have to do some code for reloading and visulizing !)\n",
    "\n",
    "#### New data \n",
    "- fef8_new_data : \n",
    "- mixed data+ aix test :  8d9a0 + b9e49 to compare bucketing impact at train time \n",
    "    - /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+bucketing-last-report-data.html or file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+bucketing-last-report-norm.html\n",
    "    - /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+no_bucketing-last-report-data.html or file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8d9a0-auxiililary_true_false.html\n",
    "    \n",
    "    \n",
    "- Auxilliary tuning : best model : b9e49-aux_report+dense+ponderatipn+no_bucketing100-dense_dim_auxilliary0.001weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_9_0f05-folderfile:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+no_bucketing-last-report-enriched-auxilliary_task_norm_not_norm-dense_dim_view.html\n",
    "-\n",
    "\n",
    "#### Last two ablations with extending epoch + mix data + ablation on auxilliary task ponderation + attention or not  \n",
    "\n",
    "- 8ce6b-extend_ep... \n",
    "    - super lame attention model (from ablation with all context)\n",
    "    - the other one a bit better   \n",
    "- a5c77 a bit better but still : no attention much better : cf. plot \n",
    "\n",
    "#### Smaller model + liu only \n",
    "- 84736 smaller model: still waiting for models : attention not helping \n",
    "\n",
    "NB : attention makes training 10 times slower  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis.html # f178\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis_norm_view.html #f178\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis_2.html # f178 + aaad \n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-norm_view.html\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-data_view.html\n",
    "# -- \n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2_best+01880_reproduction-VAL_TRUE.html\n",
    "##file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-iterate+new_data-norm_view.html (different results with below ? get_batch due ?? )\n",
    "##/Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-norm_view.html \n",
    "## diffent from file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2_best+01880_reproduction-VAL_TRUE.html val True \n",
    "#--\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/liu-attention+unrolling-more-param.html #e390\n",
    "#!open file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/e390_best+24f9d-VAL_FALSE.html (e390 with Vale False 10 points above!! )\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/fef8_new_data.html (very good ?)\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/extend_ep-SENT_context-get_True-attention_simplifiedXauxXdropout-last+bucket_False_eval-get_batch_False.html\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout.html\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8e628-no_bucketing-get_batch_False-train-attention-last+bucket_False_eval-get_batch_False-report.json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------Interatcing with new model--------------------------------\n",
      " 97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e \n",
      "\n",
      "\n",
      "Loading dictionary from ./../checkpoints/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-folder/dictionaries \n",
      "Character vocabulary is 109 length\n",
      "WARNING : checkpoint_dir as indicated in args.json is not found, lt checkpoint_dir /\n",
      "Loading model with argument {'shared_context': 'all', 'n_trainable_parameters': 661361, 'hidden_size_sent_encoder': 100, 'decoder_arch': {'cell_word': 'LSTM', 'unrolling_word': True, 'drop_out_char_embedding_decoder': 0.2, 'teacher_force': False, 'drop_out_word_decoder_cell': 0.0, 'dir_word': 'uni', 'cell_sentence': 'none', 'char_src_attention': False}, 'voc_size': 109, 'hidden_size_decoder': 200, 'batch_size': 40, 'encoder_arch': {'cell_word': 'LSTM', 'dir_word_encoder': 1, 'cell_sentence': 'LSTM', 'dir_sent_encoder': 1, 'drop_out_word_encoder_out': 0.0, 'dropout_word_encoder_cell': 0.0, 'n_layers_word_encoder': 1, 'dropout_sent_encoder_cell': 0, 'drop_out_sent_encoder_out': 0.0}, 'tasks_schedule_policy': None, 'char_embedding_dim': 50, 'hidden_size_encoder': 200, 'output_dim': 100, 'auxilliary_arch': {'auxilliary_task_norm_not_norm': True, 'auxilliary_task_norm_not_norm-dense_dim': 200, 'auxilliary_task_norm_not_norm-dense_dim_2': 0, 'weight_binary_loss': None}, 'gradient_clipping': 1}\n",
      "Model arguments are {'checkpoint_dir': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-folder/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-Xep-outof80ep-checkpoint.pt', 'hyperparameters': {'shared_context': 'all', 'n_trainable_parameters': 661361, 'hidden_size_sent_encoder': 100, 'decoder_arch': {'cell_word': 'LSTM', 'unrolling_word': True, 'drop_out_char_embedding_decoder': 0.2, 'teacher_force': False, 'drop_out_word_decoder_cell': 0.0, 'dir_word': 'uni', 'cell_sentence': 'none', 'char_src_attention': False}, 'voc_size': 109, 'hidden_size_decoder': 200, 'batch_size': 40, 'encoder_arch': {'cell_word': 'LSTM', 'dir_word_encoder': 1, 'cell_sentence': 'LSTM', 'dir_sent_encoder': 1, 'drop_out_word_encoder_out': 0.0, 'dropout_word_encoder_cell': 0.0, 'n_layers_word_encoder': 1, 'dropout_sent_encoder_cell': 0, 'drop_out_sent_encoder_out': 0.0}, 'tasks_schedule_policy': None, 'char_embedding_dim': 50, 'hidden_size_encoder': 200, 'output_dim': 100, 'auxilliary_arch': {'auxilliary_task_norm_not_norm': True, 'auxilliary_task_norm_not_norm-dense_dim': 200, 'auxilliary_task_norm_not_norm-dense_dim_2': 0, 'weight_binary_loss': None}, 'gradient_clipping': 1}, 'info_checkpoint': {'train_data_path': '/scratch/bemuller/mt_norm_parse/env/.././data/LiLiu/2577_tweets-li.conll', 'n_epochs': 18, 'dev_data_path': '/scratch/bemuller/mt_norm_parse/env/../../parsing/normpar/data/owoputi.integrated_fixed', 'other': {'error_curves_details': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-folder/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-details-last-plo-seq.png', 'extend_n_batch': 2, 'seed(np/torch)': [123, 123], 'average_per_epoch(min)': '0.06', 'time_training(min)': '4.43', 'loss': 0.0004838762646837099, 'error_curves': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-folder/97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e-last-plo-seq.png', 'data': 'dev', 'weight_binary_loss': 0.01}, 'tasks_schedule_policy': None, 'git_id': '2393f5a0eac362eeeea1d4b58bda054751c29cc6', 'teacher_force': True, 'gradient_clipping': 1, 'batch_size': 40}} \n",
      "WARNING : hidden_size of word_recurrent_cell has been divided by 1 dir_word_encoder\n",
      "WARNING : BinaryPredictor dense_dim is set to 200 in norm_not_norm predictor\n",
      "WARNING : DECODER unrolling_word is True\n",
      "WARNING : DECODER char_src_attention is False\n",
      "INFO : dictionary is None so setting char_dictionary to model.char_dictionary\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    ok\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    whats\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    up\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    friends\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['ok', 'whats', 'up', 'my', 'friends', '']\n",
      "NORMALIZING : [('NORMED', 'ok'), ('NEED_NORM', 'whats'), ('NORMED', 'up'), ('NORMED', 'my'), ('NORMED', 'friends')] \n",
      "DECODED text is : ['other', 'whates', 'up', 'tomorrow', 'friends'] original is ['ok', 'whats', 'up', 'my', 'friends']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    are\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    you\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    tellong\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    me\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    something\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    interesting\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['are', 'you', 'tellong', 'me', 'something', 'interesting', '']\n",
      "NORMALIZING : [('NORMED', 'are'), ('NORMED', 'you'), ('NORMED', 'tellong'), ('NORMED', 'me'), ('NORMED', 'something'), ('NORMED', 'interesting')] \n",
      "DECODED text is : ['ater', 'you', 'tellongure', 'temp', 'something', 'interesting'] original is ['are', 'you', 'tellong', 'me', 'something', 'interesting']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    something\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    or\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    somthin\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['something', 'or', 'somthin', '']\n",
      "NORMALIZING : [('NORMED', 'something'), ('NORMED', 'or'), ('NORMED', 'somthin')] \n",
      "DECODED text is : ['something', 'toor', 'sombotice'] original is ['something', 'or', 'somthin']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    waitin\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    for\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    yo\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['waitin', 'for', 'yo', '']\n",
      "NORMALIZING : [('NEED_NORM', 'waitin'), ('NORMED', 'for'), ('NORMED', 'yo')] \n",
      "DECODED text is : ['waiting', 'offro', 'to'] original is ['waitin', 'for', 'yo']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    waitin\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    or\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    waiting\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['waitin', 'or', 'waiting', '']\n",
      "NORMALIZING : [('NEED_NORM', 'waitin'), ('NORMED', 'or'), ('NORMED', 'waiting')] \n",
      "DECODED text is : ['waiting', 'toor', 'waiting'] original is ['waitin', 'or', 'waiting']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    yeah\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    thanx\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    u\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    I\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    will\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    b\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    gettin\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    better\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['yeah', 'thanx', '2', 'u', 'I', 'will', 'b', 'gettin', 'better', '']\n",
      "NORMALIZING : [('NEED_NORM', 'yeah'), ('NEED_NORM', 'thanx'), ('NORMED', '2'), ('NORMED', 'u'), ('NORMED', 'I'), ('NORMED', 'will'), ('NORMED', 'b'), ('NORMED', 'gettin'), ('NORMED', 'better')] \n",
      "DECODED text is : ['yeah', 'thanks', 'top', 'tou', 'the', 'will', 'tope', 'getting', 'better'] original is ['yeah', 'thanx', '2', 'u', 'I', 'will', 'b', 'gettin', 'better']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    yesss\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['yesss', '']\n",
      "NORMALIZING : [('NEED_NORM', 'yesss')] \n",
      "DECODED text is : ['yes'] original is ['yesss']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    yes\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['yes', '']\n",
      "NORMALIZING : [('NEED_NORM', 'yes')] \n",
      "DECODED text is : ['yes'] original is ['yes']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    Yes\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['Yes', '']\n",
      "NORMALIZING : [('NEED_NORM', 'Yes')] \n",
      "DECODED text is : ['yes'] original is ['Yes']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    yesss\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['yesss', '']\n",
      "NORMALIZING : [('NEED_NORM', 'yesss')] \n",
      "DECODED text is : ['yes'] original is ['yesss']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    yesssssss\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['yesssssss', '']\n",
      "NORMALIZING : [('NEED_NORM', 'yesssssss')] \n",
      "DECODED text is : ['yes'] original is ['yesssssss']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    dont\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    wana\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    go\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    to\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    work\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    today\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    !!!\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['i', 'dont', 'wana', 'go', 'to', 'work', 'today', '!!!', '']\n",
      "NORMALIZING : [('NORMED', 'i'), ('NEED_NORM', 'dont'), ('NEED_NORM', 'wana'), ('NORMED', 'go'), ('NORMED', 'to'), ('NORMED', 'work'), ('NEED_NORM', 'today'), ('NORMED', '!!!')] \n",
      "DECODED text is : ['time', \"don't\", 'thanks', 'too', 'too', 'workout', 'today', '!!!!'] original is ['i', 'dont', 'wana', 'go', 'to', 'work', 'today', '!!!']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    dont\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    do\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    this\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['dont', 'do', 'this', '']\n",
      "NORMALIZING : [('NEED_NORM', 'dont'), ('NORMED', 'do'), ('NORMED', 'this')] \n",
      "DECODED text is : [\"don't\", 'to', 'thister'] original is ['dont', 'do', 'this']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    do\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    it\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['do', 'it', '']\n",
      "NORMALIZING : [('NORMED', 'do'), ('NORMED', 'it')] \n",
      "DECODED text is : ['to', 'twitter'] original is ['do', 'it']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    it\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['it', '']\n",
      "NORMALIZING : [('NORMED', 'it')] \n",
      "DECODED text is : ['twitter'] original is ['it']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    to\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    be\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    or\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    not\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    to\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    be\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['to', 'be', 'or', 'not', 'to', 'be', '']\n",
      "NORMALIZING : [('NORMED', 'to'), ('NORMED', 'be'), ('NORMED', 'or'), ('NORMED', 'not'), ('NORMED', 'to'), ('NORMED', 'be')] \n",
      "DECODED text is : ['too', 'before', 'toor', \"ton't\", 'too', 'before'] original is ['to', 'be', 'or', 'not', 'to', 'be']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    hii\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    demo\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    !\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['hii', 'demo', '!', '']\n",
      "NORMALIZING : [('NORMED', 'hii'), ('NEED_NORM', 'demo'), ('NORMED', '!')] \n",
      "DECODED text is : ['hiha', 'does', 'if'] original is ['hii', 'demo', '!']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    answear\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    me\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    please\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['answear', 'me', 'please', '']\n",
      "NORMALIZING : [('NORMED', 'answear'), ('NORMED', 'me'), ('NORMED', 'please')] \n",
      "DECODED text is : ['anawrers', 'teme', 'please'] original is ['answear', 'me', 'please']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i'm\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    up\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    and\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i'm\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    kickin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    with\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    fam\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"i'm\", 'up', 'and', \"i'm\", 'kickin', 'with', 'my', 'fam', '']\n",
      "NORMALIZING : [('NORMED', \"i'm\"), ('NORMED', 'up'), ('NORMED', 'and'), ('NORMED', \"i'm\"), ('NORMED', 'kickin'), ('NORMED', 'with'), ('NORMED', 'my'), ('NORMED', 'fam')] \n",
      "DECODED text is : [\"i'm\", 'up', 'and', \"i'm\", 'kicking', 'without', 'tomorrow', 'famal'] original is [\"i'm\", 'up', 'and', \"i'm\", 'kickin', 'with', 'my', 'fam']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['my', '']\n",
      "NORMALIZING : [('NORMED', 'my')] \n",
      "DECODED text is : ['tomorrow'] original is ['my']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    2morrow\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['2morrow', '']\n",
      "NORMALIZING : [('NORMED', '2morrow')] \n",
      "DECODED text is : ['tomorrow'] original is ['2morrow']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    2mor\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['2mor', '']\n",
      "NORMALIZING : [('NORMED', '2mor')] \n",
      "DECODED text is : ['tomorrow'] original is ['2mor']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    her\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['her', '']\n",
      "NORMALIZING : [('NORMED', 'her')] \n",
      "DECODED text is : ['there'] original is ['her']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    he\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['he', '']\n",
      "NORMALIZING : [('NORMED', 'he')] \n",
      "DECODED text is : ['the'] original is ['he']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    his\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['his', '']\n",
      "NORMALIZING : [('NORMED', 'his')] \n",
      "DECODED text is : ['hisha'] original is ['his']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    ok\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['ok', '']\n",
      "NORMALIZING : [('NORMED', 'ok')] \n",
      "DECODED text is : ['other'] original is ['ok']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    Thx\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['Thx', '']\n",
      "NORMALIZING : [('NORMED', 'Thx')] \n",
      "DECODED text is : [\"That's\"] original is ['Thx']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    gramma\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    is\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    homr\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['gramma', 'is', 'homr', '']\n",
      "NORMALIZING : [('NORMED', 'gramma'), ('NORMED', 'is'), ('NORMED', 'homr')] \n",
      "DECODED text is : ['gramber', 'tisple', 'homorrow'] original is ['gramma', 'is', 'homr']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    I\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    wish\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    hav\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    coould\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    seen\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    it\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['I', 'wish', 'hav', 'coould', 'seen', 'it', '']\n",
      "NORMALIZING : [('NORMED', 'I'), ('NORMED', 'wish'), ('NEED_NORM', 'hav'), ('NORMED', 'coould'), ('NORMED', 'seen'), ('NORMED', 'it')] \n",
      "DECODED text is : ['the', 'wisher', \"havan't\", 'cooled', 'seens', 'twitter'] original is ['I', 'wish', 'hav', 'coould', 'seen', 'it']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    hav\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['hav', '']\n",
      "NORMALIZING : [('NEED_NORM', 'hav')] \n",
      "DECODED text is : [\"havan't\"] original is ['hav']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    incredables\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['incredables', '']\n",
      "NORMALIZING : [('NEED_NORM', 'incredables')] \n",
      "DECODED text is : ['incrander'] original is ['incredables']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    Outside\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    home\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['Outside', 'my', 'home', '']\n",
      "NORMALIZING : [('NORMED', 'Outside'), ('NORMED', 'my'), ('NORMED', 'home')] \n",
      "DECODED text is : ['Outsode', 'tomorrow', 'home'] original is ['Outside', 'my', 'home']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    errryonestop\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    errryone\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['errryonestop', 'errryone', '']\n",
      "NORMALIZING : [('NORMED', 'errryonestop'), ('NORMED', 'errryone')] \n",
      "DECODED text is : ['errounceload', 'erryonce'] original is ['errryonestop', 'errryone']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    one\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['one', '']\n",
      "NORMALIZING : [('NORMED', 'one')] \n",
      "DECODED text is : ['one'] original is ['one']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    rrr\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['rrr', '']\n",
      "NORMALIZING : [('NORMED', 'rrr')] \n",
      "DECODED text is : ['troll'] original is ['rrr']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    omg\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['omg', '']\n",
      "NORMALIZING : [('NORMED', 'omg')] \n",
      "DECODED text is : ['omg'] original is ['omg']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    feel\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    so\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    special\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['i', 'feel', 'so', 'special', '']\n",
      "NORMALIZING : [('NORMED', 'i'), ('NORMED', 'feel'), ('NORMED', 'so'), ('NORMED', 'special')] \n",
      "DECODED text is : ['time', 'feel', 'top', 'pecially'] original is ['i', 'feel', 'so', 'special']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['i', '']\n",
      "NORMALIZING : [('NORMED', 'i')] \n",
      "DECODED text is : ['time'] original is ['i']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    me\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['me', '']\n",
      "NORMALIZING : [('NORMED', 'me')] \n",
      "DECODED text is : ['teme'] original is ['me']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    luvvee\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    youu\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['luvvee', 'youu', '']\n",
      "NORMALIZING : [('NEED_NORM', 'luvvee'), ('NEED_NORM', 'youu')] \n",
      "DECODED text is : ['lovelever', 'you'] original is ['luvvee', 'youu']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    lovely\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['lovely', '']\n",
      "NORMALIZING : [('NORMED', 'lovely')] \n",
      "DECODED text is : [\"lovely's\"] original is ['lovely']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    supersticious\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['supersticious', '']\n",
      "NORMALIZING : [('NORMED', 'supersticious')] \n",
      "DECODED text is : ['suppricate'] original is ['supersticious']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    work\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    with\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    a\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    bunch\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    of\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    lame\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    people\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['i', 'work', 'with', 'a', 'bunch', 'of', 'lame', 'people', '']\n",
      "NORMALIZING : [('NORMED', 'i'), ('NORMED', 'work'), ('NORMED', 'with'), ('NORMED', 'a'), ('NORMED', 'bunch'), ('NORMED', 'of'), ('NORMED', 'lame'), ('NORMED', 'people')] \n",
      "DECODED text is : ['time', 'worker', 'twitter', 'ta', 'unchube', 'of', 'lamel', 'popele'] original is ['i', 'work', 'with', 'a', 'bunch', 'of', 'lame', 'people']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    with\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['with', '']\n",
      "NORMALIZING : [('NORMED', 'with')] \n",
      "DECODED text is : ['wither'] original is ['with']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    with\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['with', '']\n",
      "NORMALIZING : [('NORMED', 'with')] \n",
      "DECODED text is : ['wither'] original is ['with']\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    work\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    with\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['i', 'work', 'with', '']\n",
      "NORMALIZING : [('NORMED', 'i'), ('NORMED', 'work'), ('NORMED', 'with')] \n",
      "DECODED text is : ['time', 'worker', 'twitter'] original is ['i', 'work', 'with']\n"
     ]
    }
   ],
   "source": [
    "for folder_name in list_:\n",
    "    assert len(list_)>0, \"list empty\"\n",
    "    model_full_name = folder_name[:-7]\n",
    "    print(\"\\n\\n--------------------------------Interatcing with new model--------------------------------\\n\", model_full_name,\"\\n\\n\")\n",
    "    dic_path = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\", \"dictionaries\")\n",
    "    model_dir = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\")\n",
    "    interact(dic_path=dic_path, dir_model=model_dir, model_full_name=model_full_name, debug=False, \n",
    "             show_attention=False,verbose=0,\n",
    "             save_attention=False)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "- on model 97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e with auxilliary task no attention quite small model\n",
    "    - interesting case : \n",
    "        - my normalized to tomorrow : probably bcause of 2mor --> tommorow \n",
    "    - very lame at decoding very short word ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
